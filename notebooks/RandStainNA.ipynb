{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "from skimage import color\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, file_path, dataset_name, transform=None):\n",
    "        self.file_path = file_path\n",
    "        self.dataset_name = dataset_name\n",
    "        self.transform = transform\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            self.dataset_len = len(file[self.dataset_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            # Retrieve data and ensure it is a numpy array\n",
    "            data = np.array(file[self.dataset_name][idx])\n",
    "\n",
    "        # Apply the transformations\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data\n",
    "\n",
    "# Use the custom dataset\n",
    "hdf5_dataset = HDF5Dataset('/Users/daviddrexlin/Code/Master/data/pcam/camelyonpatch_level_2_split_train_x.h5-002', 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getavgstd(image):\n",
    "    avg = []\n",
    "    std = []\n",
    "    image_avg_l = np.mean(image[:, :, 0])\n",
    "    image_std_l = np.std(image[:, :, 0])\n",
    "    image_avg_a = np.mean(image[:, :, 1])\n",
    "    image_std_a = np.std(image[:, :, 1])\n",
    "    image_avg_b = np.mean(image[:, :, 2])\n",
    "    image_std_b = np.std(image[:, :, 2])\n",
    "    avg.append(image_avg_l)\n",
    "    avg.append(image_avg_a)\n",
    "    avg.append(image_avg_b)\n",
    "    std.append(image_std_l)\n",
    "    std.append(image_std_a)\n",
    "    std.append(image_std_b)\n",
    "    return (avg, std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_avg_list = [[] for _ in range(3)]  # For each channel\n",
    "lab_std_list = [[] for _ in range(3)]\n",
    "\n",
    "for idx in range(len(hdf5_dataset)):\n",
    "\n",
    "    color_space = \"HSV\"\n",
    "    img = hdf5_dataset[idx]\n",
    "    if color_space == \"LAB\":\n",
    "        img = color.rgb2lab(img)\n",
    "    elif color_space == \"HED\":\n",
    "        img = color.rgb2hed(img)\n",
    "    elif color_space == \"HSV\":\n",
    "        img = color.rgb2hsv(img)\n",
    "\n",
    "    avg, std = getavgstd(img)\n",
    "    for i in range(3):\n",
    "        lab_avg_list[i].append(avg[i])\n",
    "        lab_std_list[i].append(std[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as scipy_stats  # Renamed to avoid conflict\n",
    "\n",
    "def get_best_fit(data, distributions):\n",
    "    best_distribution = None\n",
    "    best_sse = np.inf  # Initialize the sum of squared errors to a large number\n",
    "\n",
    "    for distribution in distributions:\n",
    "        dist = getattr(scipy_stats, distribution)\n",
    "        params = dist.fit(data)\n",
    "        fitted_data = dist.pdf(np.linspace(min(data), max(data), len(data)), *params[:-2], loc=params[-2], scale=params[-1])\n",
    "        sse = np.sum((np.histogram(data, bins=len(data), density=True)[0] - fitted_data) ** 2)\n",
    "\n",
    "        if sse < best_sse:\n",
    "            best_sse = sse\n",
    "            best_distribution = distribution\n",
    "\n",
    "    return best_distribution\n",
    "\n",
    "# Assuming lab_avg_list and lab_std_list are lists of data for each channel, and color_space is defined\n",
    "\n",
    "stats = {}\n",
    "distributions = [\"norm\", \"laplace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as scipy_stats\n",
    "import yaml\n",
    "\n",
    "def get_best_fit(data, distributions):\n",
    "    best_distribution = None\n",
    "    best_sse = np.inf  # Initialize the sum of squared errors to a large number\n",
    "\n",
    "    for distribution in distributions:\n",
    "        dist = getattr(scipy_stats, distribution)\n",
    "        params = dist.fit(data)\n",
    "        fitted_data = dist.pdf(np.linspace(min(data), max(data), len(data)), *params[:-2], loc=params[-2], scale=params[-1])\n",
    "        sse = np.sum((np.histogram(data, bins=len(data), density=True)[0] - fitted_data) ** 2)\n",
    "\n",
    "        if sse < best_sse:\n",
    "            best_sse = sse\n",
    "            best_distribution = distribution\n",
    "\n",
    "    return best_distribution\n",
    "\n",
    "# Assuming lab_avg_list and lab_std_list are lists of data for each channel, and color_space is defined\n",
    "\n",
    "stats = {}\n",
    "distributions = [\"norm\", \"laplace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset statistics saved in ./dataset_statistics.yaml\n"
     ]
    }
   ],
   "source": [
    "for i, (avg_list, std_list) in enumerate(zip(lab_avg_list, lab_std_list)):\n",
    "    channel = color_space[i]\n",
    "    avg_distribution = get_best_fit(avg_list, distributions)\n",
    "    std_distribution = get_best_fit(std_list, distributions)\n",
    "\n",
    "    # Convert numpy scalar types to native Python types\n",
    "    avg_mean = round(float(np.mean(avg_list)), 3)\n",
    "    avg_std = round(float(np.std(avg_list)), 3)\n",
    "    std_mean = round(float(np.mean(std_list)), 3)\n",
    "    std_std = round(float(np.std(std_list)), 3)\n",
    "\n",
    "    stats[channel] = {\n",
    "        \"avg\": {\n",
    "            \"mean\": avg_mean,\n",
    "            \"std\": avg_std,\n",
    "            \"distribution\": avg_distribution,\n",
    "        },\n",
    "        \"std\": {\n",
    "            \"mean\": std_mean,\n",
    "            \"std\": std_std,\n",
    "            \"distribution\": std_distribution,\n",
    "        },\n",
    "    }\n",
    "\n",
    "yaml_save_path = \"./dataset_statistics.yaml\"\n",
    "with open(yaml_save_path, \"w\") as f:\n",
    "    yaml.dump(stats, f)\n",
    "\n",
    "print(f\"Dataset statistics saved in {yaml_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
