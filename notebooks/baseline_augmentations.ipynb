{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'augmentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maugmentations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m normalization\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'augmentations'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from augmentations import normalization\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import numpy as np\n",
    "from data_loader import HDF5Dataset, HDF5Dataset_Labels, MergedDataset\n",
    "import wandb \n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './hyper.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m yaml_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./hyper.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myaml_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      3\u001b[0m     hyperparameters \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(file)\n\u001b[1;32m      5\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './hyper.yaml'"
     ]
    }
   ],
   "source": [
    "yaml_file_path = './hyper.yaml'\n",
    "with open(yaml_file_path, 'r') as file:\n",
    "    hyperparameters = yaml.safe_load(file)\n",
    "\n",
    "learning_rate = hyperparameters['learning_rate']\n",
    "batch_size = hyperparameters['batch_size']\n",
    "num_epochs = hyperparameters['num_epochs']\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pytorch-lightning\n",
      "  Downloading pytorch_lightning-2.2.0.post0-py3-none-any.whl (800 kB)\n",
      "\u001b[K     |████████████████████████████████| 800 kB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.57.0 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning) (4.66.1)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning) (2023.12.2)\n",
      "Requirement already satisfied: PyYAML>=5.4 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning) (23.2)\n",
      "Collecting lightning-utilities>=0.8.0\n",
      "  Downloading lightning_utilities-0.10.1-py3-none-any.whl (24 kB)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-1.3.1-py3-none-any.whl (840 kB)\n",
      "\u001b[K     |████████████████████████████████| 840 kB 13.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.2 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning) (1.26.3)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from pytorch-lightning) (4.9.0)\n",
      "Requirement already satisfied: requests in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2.31.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.9.3-cp39-cp39-macosx_11_0_arm64.whl (388 kB)\n",
      "\u001b[K     |████████████████████████████████| 388 kB 56.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting async-timeout<5.0,>=4.0\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[K     |████████████████████████████████| 60 kB 14.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp39-cp39-macosx_11_0_arm64.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 7.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp39-cp39-macosx_11_0_arm64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 19.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /Applications/Xcode.app/Contents/Developer/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from lightning-utilities>=0.8.0->pytorch-lightning) (58.0.4)\n",
      "Requirement already satisfied: filelock in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->pytorch-lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->pytorch-lightning) (1.12)\n",
      "Requirement already satisfied: jinja2 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->pytorch-lightning) (3.1.2)\n",
      "Requirement already satisfied: networkx in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from torch>=1.13.0->pytorch-lightning) (3.2.1)\n",
      "Requirement already satisfied: idna>=2.0 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from yarl<2.0,>=1.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from jinja2->torch>=1.13.0->pytorch-lightning) (2.1.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from requests->fsspec[http]>=2022.5.0->pytorch-lightning) (3.3.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages (from sympy->torch>=1.13.0->pytorch-lightning) (1.3.0)\n",
      "Installing collected packages: multidict, frozenlist, yarl, attrs, async-timeout, aiosignal, lightning-utilities, aiohttp, torchmetrics, pytorch-lightning\n",
      "Successfully installed aiohttp-3.9.3 aiosignal-1.3.1 async-timeout-4.0.3 attrs-23.2.0 frozenlist-1.4.1 lightning-utilities-0.10.1 multidict-6.0.5 pytorch-lightning-2.2.0.post0 torchmetrics-1.3.1 yarl-1.9.4\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.0 is available.\n",
      "You should consider upgrading via the '/Applications/Xcode.app/Contents/Developer/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "/Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdrexlin-david\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /Users/daviddrexlin/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/daviddrexlin/Code/Master/wandb/run-20240229_103436-jpenwc4z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/drexlin-david/Histo_C16_No_Augmentations/runs/jpenwc4z' target=\"_blank\">stoic-violet-33</a></strong> to <a href='https://wandb.ai/drexlin-david/Histo_C16_No_Augmentations' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/drexlin-david/Histo_C16_No_Augmentations' target=\"_blank\">https://wandb.ai/drexlin-david/Histo_C16_No_Augmentations</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/drexlin-david/Histo_C16_No_Augmentations/runs/jpenwc4z' target=\"_blank\">https://wandb.ai/drexlin-david/Histo_C16_No_Augmentations/runs/jpenwc4z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.login(key='5a1e342ba066e42ad7a4054d1cfcd50ee372775a')\n",
    "wandb.init(project=\"Histo_C16_No_Augmentations\", entity=\"drexlin-david\", name=\"your_custom_run_name\")\n",
    "config = wandb.config\n",
    "config.learning_rate = learning_rate\n",
    "config.batch_size = batch_size\n",
    "config.epochs = num_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the custom dataset\n",
    "x = HDF5Dataset('./data/pcam/camelyonpatch_level_2_split_train_x.h5-002', 'x', normalization())\n",
    "y =  HDF5Dataset_Labels('./data/pcam/camelyonpatch_level_2_split_train_y.h5', 'y') \n",
    "data = MergedDataset(x,y)\n",
    "\n",
    "# Create a DataLoader\n",
    "train_loader = DataLoader(data, batch_size = batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/daviddrexlin/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the ResNet18 model\n",
    "model = models.resnet18(pretrained=False)\n",
    "# Modify the last layer for binary classification\n",
    "\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/64 [00:00<?, ?batch/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   8%|▊         | 5/64 [02:12<26:23, 26.84s/batch]"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", unit='batch'):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = torch.squeeze(labels)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        predicted = torch.sigmoid(outputs).squeeze() > 0.5\n",
    "    \n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = correct / total\n",
    "    wandb.log({\"loss\": epoch_loss, \"accuracy\": epoch_acc})\n",
    "\n",
    "    print(f\"Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}\")\n",
    "wandb.finish()\n",
    "# You can add code for testing the model on test_loader similar to the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class HDF5Dataset_Labels(Dataset):\n",
    "    def __init__(self, file_path, dataset_name):\n",
    "        self.file_path = file_path\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            self.dataset_len = len(file[self.dataset_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            data = file[self.dataset_name][idx]\n",
    "\n",
    "            return data\n",
    "\n",
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, file_path, dataset_name, transform):\n",
    "        self.file_path = file_path\n",
    "        self.dataset_name = dataset_name\n",
    "        self.transform = transform\n",
    "\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            self.dataset_len = len(file[self.dataset_name])\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        with h5py.File(self.file_path, 'r') as file:\n",
    "            data = file[self.dataset_name][idx]\n",
    "\n",
    "        # Apply the transformations\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        # Convert data to a PyTorch tensor if it's a NumPy array\n",
    "        #if isinstance(data, np.ndarray):\n",
    "        #    data = torch.from_numpy(data)\n",
    "        \n",
    "        #print(type(data))\n",
    "        # Now you can safely clone and detach\n",
    "        data = data.clone().detach()\n",
    "\n",
    "        return data\n",
    "\n",
    "class MergedDataset(Dataset):\n",
    "    def __init__(self, data_dataset, labels_dataset):\n",
    "        self.data_dataset = data_dataset\n",
    "        self.labels_dataset = labels_dataset\n",
    "        assert len(self.data_dataset) == len(self.labels_dataset), \"Datasets must be of equal length\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_dataset[idx]\n",
    "        label = self.labels_dataset[idx]\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m      9\u001b[0m transforms \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     10\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mLambda(numpy_to_pil),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#transforms.Resize((224, 224)),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# std for 224, 224 ->>> transforms.Normalize(mean=[0.7008, 0.5384, 0.6916], std=[0.2177, 0.2621, 0.1947]) \u001b[39;00m\n\u001b[1;32m     15\u001b[0m ])\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m HDF5Dataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/daviddrexlin/Code/Master/data/pcam/camelyonpatch_level_2_split_test_x.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransforms)\n\u001b[0;32m---> 18\u001b[0m y \u001b[38;5;241m=\u001b[39m  \u001b[43mHDF5Dataset_Labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./Users/daviddrexlin/Code/Master/data/pcam/camelyonpatch_level_2_split_test_y.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     19\u001b[0m data \u001b[38;5;241m=\u001b[39m MergedDataset(x,y)\n\u001b[1;32m     20\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m DataLoader(data, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2048\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'transform'"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Use the custom dataset\n",
    "def numpy_to_pil(np_array):\n",
    "    # Assuming the numpy array is an image with shape (H, W, C)\n",
    "    return Image.fromarray(np_array.astype('uint8'), 'RGB')\n",
    "\n",
    "transforms = transforms.Compose([\n",
    "    transforms.Lambda(numpy_to_pil),\n",
    "    #transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.7008, 0.5384, 0.6916], std=[0.2350, 0.2774, 0.2129])\n",
    "    # std for 224, 224 ->>> transforms.Normalize(mean=[0.7008, 0.5384, 0.6916], std=[0.2177, 0.2621, 0.1947]) \n",
    "])\n",
    "\n",
    "x = HDF5Dataset('/Users/daviddrexlin/Code/Master/data/pcam/camelyonpatch_level_2_split_test_x.h5', 'x', transform=transforms)\n",
    "y =  HDF5Dataset_Labels('./Users/daviddrexlin/Code/Master/data/pcam/camelyonpatch_level_2_split_test_y.h5', 'y', transform=transforms) \n",
    "data = MergedDataset(x,y)\n",
    "test_loader = DataLoader(data, batch_size = 2048, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:42<00:00,  2.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.7842, Accuracy: 68.9240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "total_loss = 0.0\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():  # No need to track gradients during testing\n",
    "\n",
    "    \n",
    "    for inputs, labels in tqdm(test_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = torch.squeeze(labels)\n",
    "\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        loss = criterion(outputs.squeeze(), labels.float())  # Compute loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        predicted = torch.sigmoid(outputs).squeeze() > 0.5\n",
    "        total_predictions += labels.size(0)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "accuracy = correct_predictions / total_predictions * 100\n",
    "\n",
    "print(f\"Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: avg_loss, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: accuracy})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "wandb.log({\"test_loss\": avg_loss, \"test_accuracy\": accuracy})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
